{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljZOVylzmAVm",
        "outputId": "93648968-2876-48da-a18c-7959d94527dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchdiffeq in /usr/local/lib/python3.11/dist-packages (0.2.5)\n",
            "Requirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from torchdiffeq) (2.6.0+cu124)\n",
            "Requirement already satisfied: scipy>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from torchdiffeq) (1.15.3)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy>=1.4.0->torchdiffeq) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->torchdiffeq) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.5.0->torchdiffeq) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.5.0->torchdiffeq) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchdiffeq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "tseyrBmtwRv1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchdiffeq import odeint\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.interpolate import interp1d"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Data stuff happens here\n",
        "time_res = 2000\n",
        "t_fine = np.linspace(0,76,time_res)\n",
        "\n",
        "url = 'http://people.whitman.edu/~hundledr/courses/M250F03/LynxHare.txt'\n",
        "df = pd.read_csv(url, delim_whitespace=True, header=None, index_col=0)\n",
        "df.index.name = 'Year'\n",
        "df.columns = ['Hare', 'Lynx']\n",
        "\n",
        "raw_years = df.index.to_numpy()  # [1845, ..., 1921]\n",
        "shifted_years = raw_years - raw_years[0]  # [0, ..., 76\n",
        "\n",
        "hares = df['Hare']\n",
        "lynxes = df['Lynx']\n",
        "\n",
        "hares_interp = interp1d(shifted_years, hares, kind='cubic')\n",
        "lynxes_interp = interp1d(shifted_years, lynxes, kind='cubic')\n",
        "\n",
        "hares_fine = hares_interp(t_fine)\n",
        "lynxes_fine = lynxes_interp(t_fine)\n",
        "\n",
        "data = np.array([hares_fine, lynxes_fine]).T\n",
        "\n",
        "data_norm = (data - np.mean(data, axis=0)) / np.std(data, axis=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsSgD2XfN4KH",
        "outputId": "30f17966-9291-412b-f79e-3a78efc20bd9"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-60-b3198017a5b0>:6: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
            "  df = pd.read_csv(url, delim_whitespace=True, header=None, index_col=0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "aytePDfKlthY"
      },
      "outputs": [],
      "source": [
        "class LVParamLearner(nn.Module):\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "      self.net = nn.Sequential(\n",
        "            nn.Linear(2*(time_res), 8),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(8, 4) #4 for alpha, beta, gamma and delta\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = torch.flatten(torch.tensor(x,dtype=torch.float32))\n",
        "      return self.net(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "paramlearner = LVParamLearner()\n",
        "optimizer = torch.optim.Adam(paramlearner.parameters(), lr=1e-4)"
      ],
      "metadata": {
        "id": "DVTJrcwtQu2i"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_deltas(alpha, beta, gamma, delta):\n",
        "    def dynamics(t, y):\n",
        "        return torch.stack([\n",
        "            alpha * y[0] - beta * y[0] * y[1],\n",
        "            delta * y[0] * y[1] - gamma * y[1]\n",
        "        ])\n",
        "    return dynamics"
      ],
      "metadata": {
        "id": "LKNBHW4WWtti"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y0 = data[0]\n",
        "y0 = torch.tensor(y0)\n",
        "t_fine = torch.tensor(t_fine)\n",
        "pred = 0\n",
        "for epoch in range(250):\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  params = paramlearner(data_norm)\n",
        "\n",
        "  alp,bet,gam,delt = nn.functional.softplus(params)\n",
        "  dynamics = make_deltas(alp,bet,gam,delt)\n",
        "  # deltas = torch.stack([alpha*data[0] - beta*data[0]*data[1],delta*data[0]*data[1]-gamma*data[1]])\n",
        "\n",
        "  pred = odeint(dynamics,y0,t_fine,method='rk4')\n",
        "\n",
        "  target = torch.tensor(data_norm)\n",
        "  loss = torch.mean(((pred - target)**2))\n",
        "\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  print(f\"EPoch: {epoch} and Loss:{loss}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ttU85AsEQ4UD",
        "outputId": "c95a02e6-b2f7-44c9-ccea-ef72dc4b9749"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-73-b82d520a42cb>:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  t_fine = torch.tensor(t_fine)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EPoch: 0 and Loss:12.865527267612801\n",
            "EPoch: 1 and Loss:10.248483056852615\n",
            "EPoch: 2 and Loss:8.35873350759967\n",
            "EPoch: 3 and Loss:7.167615758422247\n",
            "EPoch: 4 and Loss:6.487041130517926\n",
            "EPoch: 5 and Loss:6.129741568867155\n",
            "EPoch: 6 and Loss:5.964298021396826\n",
            "EPoch: 7 and Loss:5.873994145533908\n",
            "EPoch: 8 and Loss:5.802855086431693\n",
            "EPoch: 9 and Loss:5.74014904115012\n",
            "EPoch: 10 and Loss:5.687704441328668\n",
            "EPoch: 11 and Loss:5.651018804505705\n",
            "EPoch: 12 and Loss:5.63133380679578\n",
            "EPoch: 13 and Loss:5.622857350073271\n",
            "EPoch: 14 and Loss:5.619201963884979\n",
            "EPoch: 15 and Loss:5.616719962831043\n",
            "EPoch: 16 and Loss:5.613788682168547\n",
            "EPoch: 17 and Loss:5.609819874950152\n",
            "EPoch: 18 and Loss:5.6046675396294\n",
            "EPoch: 19 and Loss:5.598339790864825\n",
            "EPoch: 20 and Loss:5.590906457353424\n",
            "EPoch: 21 and Loss:5.582489551776922\n",
            "EPoch: 22 and Loss:5.573313035100913\n",
            "EPoch: 23 and Loss:5.563764302317908\n",
            "EPoch: 24 and Loss:5.554449759542444\n",
            "EPoch: 25 and Loss:5.546212008683689\n",
            "EPoch: 26 and Loss:5.540006573996976\n",
            "EPoch: 27 and Loss:5.536420138831533\n",
            "EPoch: 28 and Loss:5.534817180238196\n",
            "EPoch: 29 and Loss:5.5331454886439895\n",
            "EPoch: 30 and Loss:5.529361464129777\n",
            "EPoch: 31 and Loss:5.523067992020242\n",
            "EPoch: 32 and Loss:5.515537906426527\n",
            "EPoch: 33 and Loss:5.508472825532097\n",
            "EPoch: 34 and Loss:5.502828235130211\n",
            "EPoch: 35 and Loss:5.498544588687132\n",
            "EPoch: 36 and Loss:5.495013424182751\n",
            "EPoch: 37 and Loss:5.491581905189151\n",
            "EPoch: 38 and Loss:5.487797339551704\n",
            "EPoch: 39 and Loss:5.483443544138565\n",
            "EPoch: 40 and Loss:5.478512032968197\n",
            "EPoch: 41 and Loss:5.473159800439787\n",
            "EPoch: 42 and Loss:5.467668487924417\n",
            "EPoch: 43 and Loss:5.462378850187119\n",
            "EPoch: 44 and Loss:5.457582809837521\n",
            "EPoch: 45 and Loss:5.453370189574906\n",
            "EPoch: 46 and Loss:5.449520121930771\n",
            "EPoch: 47 and Loss:5.445589006803907\n",
            "EPoch: 48 and Loss:5.441213041438217\n",
            "EPoch: 49 and Loss:5.436378058113763\n",
            "EPoch: 50 and Loss:5.431383393106437\n",
            "EPoch: 51 and Loss:5.42657423062912\n",
            "EPoch: 52 and Loss:5.422106120827388\n",
            "EPoch: 53 and Loss:5.417908485370585\n",
            "EPoch: 54 and Loss:5.413794812253482\n",
            "EPoch: 55 and Loss:5.409589364230324\n",
            "EPoch: 56 and Loss:5.4052053307550105\n",
            "EPoch: 57 and Loss:5.400661154086704\n",
            "EPoch: 58 and Loss:5.396058877157213\n",
            "EPoch: 59 and Loss:5.391527907133833\n",
            "EPoch: 60 and Loss:5.387157842215467\n",
            "EPoch: 61 and Loss:5.382938613754294\n",
            "EPoch: 62 and Loss:5.378767695680527\n",
            "EPoch: 63 and Loss:5.374523600348172\n",
            "EPoch: 64 and Loss:5.370158850396502\n",
            "EPoch: 65 and Loss:5.365730630796992\n",
            "EPoch: 66 and Loss:5.361335974136418\n",
            "EPoch: 67 and Loss:5.357037151470826\n",
            "EPoch: 68 and Loss:5.352822910628267\n",
            "EPoch: 69 and Loss:5.348636057927639\n",
            "EPoch: 70 and Loss:5.344418679153051\n",
            "EPoch: 71 and Loss:5.340149505589076\n",
            "EPoch: 72 and Loss:5.335850800113359\n",
            "EPoch: 73 and Loss:5.331567780043516\n",
            "EPoch: 74 and Loss:5.327335154929367\n",
            "EPoch: 75 and Loss:5.323154408994893\n",
            "EPoch: 76 and Loss:5.318993175138654\n",
            "EPoch: 77 and Loss:5.314815226215649\n",
            "EPoch: 78 and Loss:5.310609113656086\n",
            "EPoch: 79 and Loss:5.306396149747137\n",
            "EPoch: 80 and Loss:5.302204525976181\n",
            "EPoch: 81 and Loss:5.298047520118865\n",
            "EPoch: 82 and Loss:5.293916449987501\n",
            "EPoch: 83 and Loss:5.289788620683603\n",
            "EPoch: 84 and Loss:5.285650514456623\n",
            "EPoch: 85 and Loss:5.281505133770791\n",
            "EPoch: 86 and Loss:5.27736812887369\n",
            "EPoch: 87 and Loss:5.273250653830673\n",
            "EPoch: 88 and Loss:5.269154561293718\n",
            "EPoch: 89 and Loss:5.265069569890049\n",
            "EPoch: 90 and Loss:5.260983387469716\n",
            "EPoch: 91 and Loss:5.256894998960688\n",
            "EPoch: 92 and Loss:5.252810884953562\n",
            "EPoch: 93 and Loss:5.248740578625746\n",
            "EPoch: 94 and Loss:5.244686005063003\n",
            "EPoch: 95 and Loss:5.240641821033818\n",
            "EPoch: 96 and Loss:5.236600955964268\n",
            "EPoch: 97 and Loss:5.2325621722711\n",
            "EPoch: 98 and Loss:5.228528690831001\n",
            "EPoch: 99 and Loss:5.224504503296938\n",
            "EPoch: 100 and Loss:5.220493281964003\n",
            "EPoch: 101 and Loss:5.21649063117035\n",
            "EPoch: 102 and Loss:5.212493432708416\n",
            "EPoch: 103 and Loss:5.208499851368855\n",
            "EPoch: 104 and Loss:5.204512814588683\n",
            "EPoch: 105 and Loss:5.2005340012871555\n",
            "EPoch: 106 and Loss:5.196565039619704\n",
            "EPoch: 107 and Loss:5.192604292840513\n",
            "EPoch: 108 and Loss:5.188649352218534\n",
            "EPoch: 109 and Loss:5.184698858278016\n",
            "EPoch: 110 and Loss:5.1807555663951765\n",
            "EPoch: 111 and Loss:5.176820145362566\n",
            "EPoch: 112 and Loss:5.172892808268518\n",
            "EPoch: 113 and Loss:5.168972741316191\n",
            "EPoch: 114 and Loss:5.165058689653284\n",
            "EPoch: 115 and Loss:5.161150554457948\n",
            "EPoch: 116 and Loss:5.157248708852677\n",
            "EPoch: 117 and Loss:5.153354983301328\n",
            "EPoch: 118 and Loss:5.149468069692075\n",
            "EPoch: 119 and Loss:5.145587847811201\n",
            "EPoch: 120 and Loss:5.141713478240997\n",
            "EPoch: 121 and Loss:5.1378459584537195\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-b82d520a42cb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m   \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m   \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m   \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(pred)\n",
        "print(data_norm)\n",
        "print(nn.functional.softplus(paramlearner(data)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6JvF70m3gkv",
        "outputId": "8d07e2d1-d045-484a-e9ff-2d1d51cb218e"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.9580e+01, 3.0090e+01],\n",
            "        [6.3921e+00, 3.4763e+01],\n",
            "        [1.9966e+00, 3.5213e+01],\n",
            "        ...,\n",
            "        [6.9626e-01, 2.7520e-38],\n",
            "        [7.0572e-01, 2.6600e-38],\n",
            "        [7.1532e-01, 2.5715e-38]], dtype=torch.float64, grad_fn=<CopySlices>)\n",
            "[[-0.74064596  0.10569586]\n",
            " [-0.74747527  0.14331119]\n",
            " [-0.7535079   0.18037613]\n",
            " ...\n",
            " [ 0.63965921 -0.96838023]\n",
            " [ 0.64484963 -0.95128741]\n",
            " [ 0.65044458 -0.93391234]]\n",
            "tensor([0.3785, 0.9234, 1.1821, 0.4060], grad_fn=<SoftplusBackward0>)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}